"""
URL ë¦¬ìŠ¤íŠ¸ ê¸°ë°˜ ë§¤ê²½ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
íŠ¹ì • URLë“¤ì„ ì§ì ‘ í¬ë¡¤ë§
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import re
import json
import csv
import os
import sys
from typing import List, Dict, Optional, Any
from urllib.parse import urlparse
import logging

# ë¡œê¹… ì„¤ì •
log_file = 'mk_crawler_link.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8', mode='a'),
        logging.StreamHandler()
    ],
    force=True
)
logger = logging.getLogger(__name__)

sys.stdout.reconfigure(encoding='utf-8')


class MKLinkCrawler:
    """ë§¤ê²½ ë‰´ìŠ¤ URL ê¸°ë°˜ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, delay: float = 0.3):
        """
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        self.base_url = "https://www.mk.co.kr"
        self.delay = delay
        self.session = requests.Session()
        
        # User-Agent ì„¤ì •
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.mk.co.kr/'
        })
        
        self.stats = {
            'total_requests': 0,
            'success_count': 0,
            'failed_count': 0
        }
    
    def extract_category_and_id(self, url: str) -> tuple:
        """URLì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ ê¸°ì‚¬ ID ì¶”ì¶œ"""
        # ì˜ˆ: https://www.mk.co.kr/news/economy/11485397
        parsed = urlparse(url)
        parts = parsed.path.strip('/').split('/')
        
        if len(parts) >= 3 and parts[0] == 'news':
            category = parts[1]
            article_id = parts[2]
            return category, article_id
        return None, None
    
    def get_article_by_url(self, url: str) -> Optional[Dict[str, Any]]:
        """
        URLë¡œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            url: ê¸°ì‚¬ URL
            
        Returns:
            ê¸°ì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        try:
            self.stats['total_requests'] += 1
            
            print(f"  ğŸ” í¬ë¡¤ë§: {url}")
            logger.info(f"í¬ë¡¤ë§: {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'lxml')
            
            # ì œëª© ì¶”ì¶œ
            title = self._extract_title(soup)
            if not title:
                logger.warning(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
                self.stats['failed_count'] += 1
                return None
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content = self._extract_content(soup)
            if not content or len(content.strip()) < 100:
                logger.warning(f"ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {url}")
                self.stats['failed_count'] += 1
                return None
            
            # URLì—ì„œ ì¹´í…Œê³ ë¦¬ì™€ ID ì¶”ì¶œ
            category_slug, article_id = self.extract_category_and_id(url)
            
            # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
            article = {
                'article_id': article_id,
                'title': title,
                'subtitle': self._extract_subtitle(soup),
                'content': content,
                'published_at': self._extract_published_date(soup),
                'category': self._get_category_name(category_slug) if category_slug else "",
                'category_slug': category_slug or "",
                'reporter': self._extract_reporter(soup),
                'image_url': self._extract_main_image(soup),
                'url': url,
                'crawled_at': datetime.now().isoformat()
            }
            
            self.stats['success_count'] += 1
            print(f"  âœ… ì„±ê³µ: {title}")
            return article
            
        except requests.exceptions.HTTPError as e:
            logger.error(f"HTTP ì—ëŸ¬ ({url}): {e}")
            self.stats['failed_count'] += 1
            return None
        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì—ëŸ¬ ({url}): {e}")
            self.stats['failed_count'] += 1
            return None
    
    def crawl_urls(self, urls: List[str]) -> List[Dict[str, Any]]:
        """
        URL ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§
        
        Args:
            urls: í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
            
        Returns:
            í¬ë¡¤ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“° URL ë¦¬ìŠ¤íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ì´ {len(urls)}ê°œ URL")
        print(f"{'='*70}\n")
        
        articles = []
        
        for i, url in enumerate(urls, 1):
            print(f"\n[{i}/{len(urls)}]")
            article = self.get_article_by_url(url)
            
            if article:
                articles.append(article)
            
            # ë§ˆì§€ë§‰ URLì´ ì•„ë‹ˆë©´ ëŒ€ê¸°
            if i < len(urls):
                time.sleep(self.delay)
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*70}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"{'='*70}")
        print(f"ì´ ìš”ì²­: {self.stats['total_requests']}ê°œ")
        print(f"ì„±ê³µ: {self.stats['success_count']}ê°œ")
        print(f"ì‹¤íŒ¨: {self.stats['failed_count']}ê°œ")
        print(f"{'='*70}\n")
        
        return articles
    
    def _get_category_name(self, category_slug: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ìŠ¬ëŸ¬ê·¸ë¥¼ í•œê¸€ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        category_map = {
            "economy": "ê²½ì œ",
            "politics": "ì •ì¹˜",
            "society": "ì‚¬íšŒ",
            "world": "êµ­ì œ",
            "business": "ê¸°ì—…",
            "stock": "ì¦ê¶Œ",
            "realestate": "ë¶€ë™ì‚°",
            "it": "IT",
            "culture": "ë¬¸í™”",
            "sports": "ìŠ¤í¬ì¸ ",
            "hot-issues": "í•«ì´ìŠˆ"
        }
        return category_map.get(category_slug, category_slug)
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        title_selectors = [
            '.news_ttl',
            'h1.news_ttl',
            '.article_headline',
            'h1.article_title',
            'h1.title',
            'h1',
            'meta[property="og:title"]'
        ]
        
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector.startswith('meta'):
                    title = elem.get('content', '').strip()
                else:
                    title = elem.get_text(strip=True)
                if title:
                    return title
        return ""
    
    def _extract_subtitle(self, soup: BeautifulSoup) -> str:
        """ë¶€ì œëª© ì¶”ì¶œ"""
        subtitle_selectors = [
            '.news_sub_ttl',
            '.article_subtitle',
            '.subtitle',
            '.summary'
        ]
        
        for selector in subtitle_selectors:
            elem = soup.select_one(selector)
            if elem:
                subtitle = elem.get_text(strip=True)
                if subtitle:
                    return subtitle
        return ""
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup.select('script, style, aside, .ad_wrap, .advertisement, .social_share, .comment'):
            tag.decompose()
        
        content_selectors = [
            '.news_cnt_detail_wrap',
            '.article_body',
            '.news_content',
            '.article_content',
            '.content_body',
            'article .content',
            '.story_body',
            '#articleBody'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator='\n', strip=True)
                content = re.sub(r'\n{3,}', '\n\n', content)
                if len(content) > 100:
                    return content
        
        # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° p íƒœê·¸ë“¤ë¡œ ì‹œë„
        paragraphs = soup.select('article p, .article p, .content p')
        if paragraphs:
            content_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            content = '\n\n'.join(content_parts)
            if len(content) > 100:
                return content
        
        return ""
    
    def _extract_published_date(self, soup: BeautifulSoup) -> str:
        """ì‘ì„± ì‹œê°„ ì¶”ì¶œ"""
        date_selectors = [
            '.news_date',
            '.article_date',
            '.published_date',
            'time[datetime]',
            '.date',
            'meta[property="article:published_time"]',
            'meta[name="publish-date"]'
        ]
        
        for selector in date_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector.startswith('meta'):
                    date_str = elem.get('content', '').strip()
                elif selector.startswith('time'):
                    date_str = elem.get('datetime', '').strip() or elem.get_text(strip=True)
                else:
                    date_str = elem.get_text(strip=True)
                
                if date_str:
                    return date_str
        return ""
    
    def _extract_reporter(self, soup: BeautifulSoup) -> str:
        """ê¸°ì ì¶”ì¶œ"""
        reporter_selectors = [
            '.journalist_name',
            '.reporter',
            '.author',
            '.writer',
            'meta[name="author"]'
        ]
        
        for selector in reporter_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector.startswith('meta'):
                    reporter = elem.get('content', '').strip()
                else:
                    reporter = elem.get_text(strip=True)
                if reporter:
                    reporter = re.sub(r'\s+', ' ', reporter).strip()
                    return reporter
        return ""
    
    def _extract_main_image(self, soup: BeautifulSoup) -> str:
        """ë©”ì¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        from urllib.parse import urljoin
        
        # og:image ë©”íƒ€ íƒœê·¸
        og_image = soup.select_one('meta[property="og:image"]')
        if og_image:
            img_url = og_image.get('content', '').strip()
            if img_url:
                return img_url
        
        # ë³¸ë¬¸ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
        img_selectors = [
            '.news_cnt_detail_wrap img',
            '.article_body img',
            'article img',
            '.content img'
        ]
        
        for selector in img_selectors:
            img = soup.select_one(selector)
            if img and img.get('src'):
                src = img.get('src', '')
                if src.startswith('http'):
                    return src
                elif src.startswith('//'):
                    return 'https:' + src
                else:
                    return urljoin(self.base_url, src)
        
        return ""


def save_to_json(articles: List[Dict[str, Any]], filename: str = None, output_dir: str = "../DB/crawling"):
    """JSON íŒŒì¼ë¡œ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"mk_news_links_{timestamp}.json"
    
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)
    
    logger.info(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath} ({len(articles)}ê°œ ê¸°ì‚¬)")
    print(f"âœ… JSON íŒŒì¼ ì €ì¥: {filepath}")
    return filepath


def save_to_csv(articles: List[Dict[str, Any]], filename: str = None, output_dir: str = "../DB/crawling"):
    """CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not articles:
        logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None
    
    os.makedirs(output_dir, exist_ok=True)
    
    if filename is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"mk_news_links_{timestamp}.csv"
    
    filepath = os.path.join(output_dir, filename)
    
    # CSV í•„ë“œ ì •ì˜
    fieldnames = ['article_id', 'title', 'subtitle', 'category', 'category_slug', 
                 'reporter', 'published_at', 'url', 'image_url', 'content', 'crawled_at']
    
    with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
        writer.writeheader()
        
        for article in articles:
            writer.writerow(article)
    
    logger.info(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath} ({len(articles)}ê°œ ê¸°ì‚¬)")
    print(f"âœ… CSV íŒŒì¼ ì €ì¥: {filepath}")
    return filepath


def main():
    # í¬ë¡¤ë§í•  URL ë¦¬ìŠ¤íŠ¸
    urls = [
        "https://www.mk.co.kr/news/economy/11485397",
        "https://www.mk.co.kr/news/politics/11485490",
        "https://www.mk.co.kr/news/it/11485059",
        "https://www.mk.co.kr/news/it/11485458",
        "https://www.mk.co.kr/news/world/11485582",
        "https://www.mk.co.kr/news/hot-issues/11485576",
        "https://www.mk.co.kr/news/economy/11485396",
        "https://www.mk.co.kr/news/politics/11485596"
    ]
    
    print("=" * 70)
    print("ë§¤ê²½ ë‰´ìŠ¤ URL í¬ë¡¤ëŸ¬ ì‹œì‘")
    print(f"ë¡œê·¸ íŒŒì¼: {os.path.abspath(log_file)}")
    print("=" * 70)
    
    # í¬ë¡¤ëŸ¬ ìƒì„± ë° ì‹¤í–‰
    crawler = MKLinkCrawler(delay=0.3)
    
    try:
        articles = crawler.crawl_urls(urls)
        
        # ê²°ê³¼ ì €ì¥
        if articles:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            json_file = save_to_json(articles, f"mk_news_links_{timestamp}.json")
            csv_file = save_to_csv(articles, f"mk_news_links_{timestamp}.csv")
            
            print(f"\n{'='*70}")
            print(f"ğŸ“ ì €ì¥ ì™„ë£Œ")
            print(f"{'='*70}")
            print(f"  JSON: {json_file}")
            print(f"  CSV: {csv_file}")
            print(f"{'='*70}\n")
        else:
            print("\nâŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
