"""
KoSimCSE ê¸°ë°˜ ë²¡í„° ì„ë² ë”© ìƒì„±
í¬ë¡¤ë§ëœ ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„°ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
"""

import os
import json
import sys
import numpy as np
from typing import List, Dict, Any
from pathlib import Path
import logging
from tqdm import tqdm

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

sys.stdout.reconfigure(encoding='utf-8')

# KoSimCSE ëª¨ë¸ ë¡œë“œ
try:
    from transformers import AutoModel, AutoTokenizer
    import torch
    
    MODEL_NAME = "BM-K/KoSimCSE-roberta"
    logger.info(f"KoSimCSE ëª¨ë¸ ë¡œë”© ì¤‘: {MODEL_NAME}")
    
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModel.from_pretrained(MODEL_NAME)
    model.eval()  # í‰ê°€ ëª¨ë“œ
    
    # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    logger.info(f"âœ… ëª¨ë¸ ë¡œë”© ì™„ë£Œ (ë””ë°”ì´ìŠ¤: {device})")
    
except Exception as e:
    logger.error(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    logger.error("í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜: pip install transformers torch")
    sys.exit(1)


def get_embedding(text: str, max_length: int = 512) -> np.ndarray:
    """
    í…ìŠ¤íŠ¸ë¥¼ ë²¡í„° ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    
    Args:
        text: ì…ë ¥ í…ìŠ¤íŠ¸
        max_length: ìµœëŒ€ í† í° ê¸¸ì´
        
    Returns:
        ë²¡í„° ì„ë² ë”© (numpy array)
    """
    if not text or not text.strip():
        # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” 0 ë²¡í„° ë°˜í™˜
        return np.zeros(model.config.hidden_size)
    
    try:
        # í…ìŠ¤íŠ¸ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(
            text,
            return_tensors="pt",
            max_length=max_length,
            truncation=True,
            padding=True
        )
        
        # GPUë¡œ ì´ë™
        inputs = {k: v.to(device) for k, v in inputs.items()}
        
        # ì„ë² ë”© ìƒì„± (gradient ê³„ì‚° ë¶ˆí•„ìš”)
        with torch.no_grad():
            outputs = model(**inputs)
            # [CLS] í† í° ì‚¬ìš© (ì²« ë²ˆì§¸ í† í°)
            embedding = outputs.last_hidden_state[:, 0, :].squeeze()
        
        # CPUë¡œ ì´ë™ í›„ numpyë¡œ ë³€í™˜
        if embedding.dim() == 0:
            embedding = embedding.unsqueeze(0)
        embedding = embedding.cpu().numpy()
        
        # ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°ì„ ìœ„í•´)
        norm = np.linalg.norm(embedding)
        if norm > 0:
            embedding = embedding / norm
        
        return embedding
        
    except Exception as e:
        logger.error(f"ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
        return np.zeros(model.config.hidden_size)


def prepare_text_for_embedding(article: Dict[str, Any]) -> str:
    """
    ê¸°ì‚¬ ë°ì´í„°ì—ì„œ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        article: ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬
        
    Returns:
        ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ (ì œëª© + ë¶€ì œëª© + ë³¸ë¬¸)
    """
    title = article.get('title', '').strip()
    subtitle = article.get('subtitle', '').strip()
    content = article.get('content', '').strip()
    
    # ì œëª© + ë¶€ì œëª© + ë³¸ë¬¸ ì¡°í•©
    parts = []
    if title:
        parts.append(title)
    if subtitle:
        parts.append(subtitle)
    if content:
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
        content_preview = content[:2000] if len(content) > 2000 else content
        parts.append(content_preview)
    
    return " ".join(parts)


def load_articles_from_json(json_file: str) -> List[Dict[str, Any]]:
    """
    JSON íŒŒì¼ì—ì„œ ê¸°ì‚¬ ë°ì´í„° ë¡œë“œ
    
    Args:
        json_file: JSON íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    try:
        with open(json_file, 'r', encoding='utf-8') as f:
            articles = json.load(f)
        
        if not isinstance(articles, list):
            articles = [articles]
        
        logger.info(f"âœ… {json_file}: {len(articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ")
        return articles
        
    except Exception as e:
        logger.error(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({json_file}): {e}")
        return []


def generate_embeddings_for_articles(articles: List[Dict[str, Any]], 
                                     batch_size: int = 8) -> List[Dict[str, Any]]:
    """
    ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì— ëŒ€í•´ ë²¡í„° ì„ë² ë”© ìƒì„±
    
    Args:
        articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        batch_size: ë°°ì¹˜ í¬ê¸° (ì„ íƒì‚¬í•­, í˜„ì¬ëŠ” 1ê°œì”© ì²˜ë¦¬)
        
    Returns:
        ì„ë² ë”©ì´ ì¶”ê°€ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    articles_with_embeddings = []
    
    logger.info(f"ğŸ“Š {len(articles)}ê°œ ê¸°ì‚¬ ì„ë² ë”© ìƒì„± ì‹œì‘...")
    
    for i, article in enumerate(tqdm(articles, desc="ì„ë² ë”© ìƒì„±")):
        try:
            # ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ì¤€ë¹„
            text = prepare_text_for_embedding(article)
            
            if not text or len(text.strip()) < 10:
                logger.warning(f"  âš ï¸  ê¸°ì‚¬ {i+1}: í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ ìŠ¤í‚µ")
                continue
            
            # ë²¡í„° ì„ë² ë”© ìƒì„±
            embedding = get_embedding(text)
            
            # ê¸°ì‚¬ì— ì„ë² ë”© ì¶”ê°€
            article_with_embedding = article.copy()
            article_with_embedding['embedding'] = embedding.tolist()
            article_with_embedding['embedding_dim'] = len(embedding)
            
            articles_with_embeddings.append(article_with_embedding)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ê°œë§ˆë‹¤)
            if (i + 1) % 10 == 0:
                logger.info(f"  âœ… {i+1}/{len(articles)}ê°œ ì™„ë£Œ")
                
        except Exception as e:
            logger.error(f"  âŒ ê¸°ì‚¬ {i+1} ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
            continue
    
    logger.info(f"âœ… ì´ {len(articles_with_embeddings)}ê°œ ê¸°ì‚¬ ì„ë² ë”© ìƒì„± ì™„ë£Œ")
    return articles_with_embeddings


def process_all_crawled_data(data_dir: str = "crawled_data") -> List[Dict[str, Any]]:
    """
    í¬ë¡¤ë§ëœ ëª¨ë“  JSON íŒŒì¼ì„ ì²˜ë¦¬í•˜ì—¬ ì„ë² ë”© ìƒì„±
    
    Args:
        data_dir: ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ
        
    Returns:
        ëª¨ë“  ê¸°ì‚¬ì˜ ì„ë² ë”©ì´ í¬í•¨ëœ ë¦¬ìŠ¤íŠ¸
    """
    data_path = Path(data_dir)
    
    if not data_path.exists():
        logger.error(f"âŒ ë°ì´í„° ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return []
    
    # ëª¨ë“  JSON íŒŒì¼ ì°¾ê¸°
    json_files = list(data_path.glob("mk_news_*.json"))
    
    if not json_files:
        logger.error(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
        return []
    
    logger.info(f"ğŸ“ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")
    
    all_articles = []
    
    # ê° íŒŒì¼ì—ì„œ ê¸°ì‚¬ ë¡œë“œ
    for json_file in json_files:
        articles = load_articles_from_json(str(json_file))
        all_articles.extend(articles)
    
    logger.info(f"ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ")
    
    # ì¤‘ë³µ ì œê±° (article_id ê¸°ì¤€)
    seen_ids = set()
    unique_articles = []
    for article in all_articles:
        article_id = article.get('article_id')
        if article_id and article_id not in seen_ids:
            seen_ids.add(article_id)
            unique_articles.append(article)
    
    logger.info(f"ğŸ“Š ì¤‘ë³µ ì œê±° í›„: {len(unique_articles)}ê°œ ê¸°ì‚¬")
    
    # ì„ë² ë”© ìƒì„±
    articles_with_embeddings = generate_embeddings_for_articles(unique_articles)
    
    return articles_with_embeddings


def save_embeddings(articles_with_embeddings: List[Dict[str, Any]], 
                   output_file: str = "embeddings.json"):
    """
    ì„ë² ë”©ì´ í¬í•¨ëœ ê¸°ì‚¬ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    
    Args:
        articles_with_embeddings: ì„ë² ë”©ì´ í¬í•¨ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    try:
        # ì„ë² ë”©ë§Œ ë³„ë„ë¡œ ì €ì¥ (ìš©ëŸ‰ ì ˆì•½)
        embeddings_data = {
            'articles': articles_with_embeddings,
            'total_count': len(articles_with_embeddings),
            'embedding_dim': articles_with_embeddings[0]['embedding_dim'] if articles_with_embeddings else 0,
            'model_name': MODEL_NAME
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(embeddings_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… ì„ë² ë”© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_file}")
        logger.info(f"   ì´ {len(articles_with_embeddings)}ê°œ ê¸°ì‚¬")
        logger.info(f"   ì„ë² ë”© ì°¨ì›: {embeddings_data['embedding_dim']}")
        
        # íŒŒì¼ í¬ê¸° í™•ì¸
        file_size = os.path.getsize(output_file) / (1024 * 1024)  # MB
        logger.info(f"   íŒŒì¼ í¬ê¸°: {file_size:.2f} MB")
        
    except Exception as e:
        logger.error(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 70)
    print("KoSimCSE ê¸°ë°˜ ë²¡í„° ì„ë² ë”© ìƒì„±")
    print("=" * 70)
    print()
    
    # í¬ë¡¤ë§ëœ ë°ì´í„° ì²˜ë¦¬
    articles_with_embeddings = process_all_crawled_data()
    
    if not articles_with_embeddings:
        logger.error("âŒ ì²˜ë¦¬í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # ì„ë² ë”© ë°ì´í„° ì €ì¥
    save_embeddings(articles_with_embeddings, "embeddings.json")
    
    # í†µê³„ ì¶œë ¥
    print("\n" + "=" * 70)
    print("ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
    print("=" * 70)
    print(f"ì´ ê¸°ì‚¬ ìˆ˜: {len(articles_with_embeddings)}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
    category_count = {}
    for article in articles_with_embeddings:
        category = article.get('category', 'Unknown')
        category_count[category] = category_count.get(category, 0) + 1
    
    print("\nì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜:")
    for category, count in sorted(category_count.items()):
        print(f"  {category:10s}: {count:4d}ê°œ")
    
    print("=" * 70)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        sys.exit(0)
    except Exception as e:
        logger.error(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)
