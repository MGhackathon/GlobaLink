"""
ë§¤ì¼ê²½ì œ(ë§¤ê²½) ë‰´ìŠ¤ í¬ë¡¤ëŸ¬
ì¹´í…Œê³ ë¦¬ë³„ ê¸°ì‚¬ ìˆ˜ì§‘ ë° íŒŒì¼/DB ì €ì¥
ë²ˆí˜¸ ê¸°ë°˜ í¬ë¡¤ë§ ì§€ì›
"""

import requests
from bs4 import BeautifulSoup
from datetime import datetime
import time
import re
import json
import csv
import os
import sys
import argparse
from typing import List, Dict, Optional, Any
from urllib.parse import urljoin, urlparse
import logging

# ë¡œê¹… ì„¤ì •
log_file = 'mk_crawler.log'
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, encoding='utf-8', mode='a'),  # append ëª¨ë“œ
        logging.StreamHandler()
    ],
    force=True  # ê¸°ì¡´ í•¸ë“¤ëŸ¬ ë®ì–´ì“°ê¸°
)
logger = logging.getLogger(__name__)
logger.info(f"ë¡œê·¸ íŒŒì¼: {os.path.abspath(log_file)}")

sys.stdout.reconfigure(encoding='utf-8')

# ë§¤ê²½ ì¹´í…Œê³ ë¦¬ ë§¤í•‘ (í•œê¸€ëª…: ìŠ¬ëŸ¬ê·¸)
MK_CATEGORIES = {
    "ê²½ì œ": "economy",
    "ì •ì¹˜": "politics",
    "ì‚¬íšŒ": "society",
    "êµ­ì œ": "world",
    "ê¸°ì—…": "business",
    "ì¦ê¶Œ": "stock",
    "ë¶€ë™ì‚°": "realestate",
    "IT": "it",
    "ë¬¸í™”": "culture",
    "ìŠ¤í¬ì¸ ": "sports"
}


class MKCrawler:
    """ë§¤ê²½ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ (ë²ˆí˜¸ ê¸°ë°˜ í¬ë¡¤ë§ ì§€ì›)"""
    
    def __init__(self, delay: float = 0.5, max_retries: int = 3, backoff_factor: float = 2.0):
        """
        Args:
            delay: ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ, ì„œë²„ ë¶€í•˜ ë°©ì§€)
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (429 ë“± ì—ëŸ¬ ì‹œ)
            backoff_factor: ì¬ì‹œë„ ì‹œ ëŒ€ê¸° ì‹œê°„ ë°°ìˆ˜ (ì§€ìˆ˜ ë°±ì˜¤í”„)
        """
        self.base_url = "https://www.mk.co.kr"
        self.delay = delay
        self.max_retries = max_retries
        self.backoff_factor = backoff_factor
        self.session = requests.Session()
        
        # User-Agent ì„¤ì • (ë§¤ê²½ robots.txt ì¤€ìˆ˜)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Referer': 'https://www.mk.co.kr/'
        })
        
        # í¬ë¡¤ë§ëœ ê¸°ì‚¬ URL ì¶”ì  (ì¤‘ë³µ ë°©ì§€)
        self.crawled_urls = set()
        
        # ìš”ì²­ í†µê³„
        self.stats = {
            'total_requests': 0,
            'success_count': 0,
            'not_found_count': 0,
            'error_count': 0
        }
    
    def get_article_by_number(self, category: str, article_id: int) -> Optional[Dict[str, Any]]:
        """
        ë²ˆí˜¸ë¡œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ ìŠ¬ëŸ¬ê·¸ (ì˜ˆ: "economy", "stock")
            article_id: ê¸°ì‚¬ ë²ˆí˜¸
            
        Returns:
            ê¸°ì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë˜ëŠ” None
        """
        url = f"{self.base_url}/news/{category}/{article_id}"
        
        for attempt in range(self.max_retries):
            try:
                self.stats['total_requests'] += 1
                response = self.session.get(url, timeout=10)
                
                # 404 Not Found - í•´ë‹¹ ë²ˆí˜¸ì˜ ê¸°ì‚¬ê°€ ì—†ìŒ
                if response.status_code == 404:
                    self.stats['not_found_count'] += 1
                    return None
                
                response.raise_for_status()
                response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'lxml')
                
                # ì œëª© ì¶”ì¶œ
                title = self._extract_title(soup)
                if not title:
                    logger.debug(f"ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {url}")
                    return None
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content = self._extract_content(soup)
                if not content or len(content.strip()) < 100:
                    logger.debug(f"ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìŒ: {url}")
                    return None
                
                # ê¸°ì‚¬ ì •ë³´ ì¶”ì¶œ
                article = {
                    'article_id': str(article_id),
                    'title': title,
                    'subtitle': self._extract_subtitle(soup),
                    'content': content,
                    'published_at': self._extract_published_date(soup),
                    'category': self._get_category_name(category),
                    'category_slug': category,
                    'reporter': self._extract_reporter(soup),
                    'image_url': self._extract_main_image(soup),
                    'url': url,
                    'crawled_at': datetime.now().isoformat()
                }
                
                self.stats['success_count'] += 1
                return article
                
            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 404:
                    self.stats['not_found_count'] += 1
                    return None
                logger.warning(f"HTTP ì—ëŸ¬ ({url}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.delay * (attempt + 1))
                    continue
                self.stats['error_count'] += 1
                return None
                
            except Exception as e:
                logger.error(f"ê¸°ì‚¬ í¬ë¡¤ë§ ì—ëŸ¬ ({url}): {e}")
                if attempt < self.max_retries - 1:
                    time.sleep(self.delay * (attempt + 1))
                    continue
                self.stats['error_count'] += 1
                return None
        
        return None
    
    def crawl_by_number_range(self, category: str, start_number: int, end_number: int = None,
                              max_articles: int = None) -> List[Dict[str, Any]]:
        """
        ë²ˆí˜¸ ë²”ìœ„ë¡œ í¬ë¡¤ë§
        
        Args:
            category: ì¹´í…Œê³ ë¦¬ ìŠ¬ëŸ¬ê·¸ (ì˜ˆ: "economy", "stock")
            start_number: ì‹œì‘ ë²ˆí˜¸ (í° ë²ˆí˜¸, ìµœì‹ )
            end_number: ì¢…ë£Œ ë²ˆí˜¸ (ì‘ì€ ë²ˆí˜¸, ê³¼ê±°). Noneì´ë©´ max_articlesë§Œí¼ë§Œ ìˆ˜ì§‘
            max_articles: ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜ (Noneì´ë©´ ë²”ìœ„ ëê¹Œì§€)
            
        Returns:
            í¬ë¡¤ë§ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        """
        category_name = self._get_category_name(category)
        
        print(f"\n{'='*70}")
        print(f"ğŸ“° [{category_name}] ë²ˆí˜¸ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘")
        print(f"ì¹´í…Œê³ ë¦¬: {category} ({category_name})")
        print(f"ì‹œì‘ ë²ˆí˜¸ (ìµœì‹ ): {start_number}")
        if end_number:
            print(f"ì¢…ë£Œ ë²ˆí˜¸ (ê³¼ê±°): {end_number}")
            print(f"ë²ˆí˜¸ ë²”ìœ„: {end_number} ~ {start_number} (ë²ˆí˜¸ë¥¼ ì¤„ì—¬ê°€ë©° ì—­ìˆœ í¬ë¡¤ë§)")
        if max_articles:
            print(f"ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {max_articles}ê°œ")
        print(f"{'='*70}")
        sys.stdout.flush()
        logger.info(f"\n{'='*70}")
        logger.info(f"ğŸ“° [{category_name}] ë²ˆí˜¸ ê¸°ë°˜ í¬ë¡¤ë§ ì‹œì‘")
        logger.info(f"ì¹´í…Œê³ ë¦¬: {category} ({category_name})")
        logger.info(f"ì‹œì‘ ë²ˆí˜¸: {start_number}")
        if end_number:
            logger.info(f"ì¢…ë£Œ ë²ˆí˜¸: {end_number}")
            logger.info(f"ë²ˆí˜¸ ë²”ìœ„: {end_number} ~ {start_number} (ì—­ìˆœ í¬ë¡¤ë§)")
        if max_articles:
            logger.info(f"ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜: {max_articles}ê°œ")
        logger.info(f"{'='*70}")
        
        articles = []
        current_number = start_number
        total_checked = 0
        
        # end_numberê°€ ì—†ìœ¼ë©´ max_articles ê¸°ë°˜ìœ¼ë¡œ ì¶”ì • (ì—°ì† 50ê°œ ì—†ìœ¼ë©´ ì¤‘ë‹¨)
        if end_number is None:
            consecutive_not_found = 0
            max_consecutive_not_found = 50
            
            while max_articles is None or len(articles) < max_articles:
                article = self.get_article_by_number(category, current_number)
                total_checked += 1
                
                if article:
                    articles.append(article)
                    consecutive_not_found = 0
                    
                    if len(articles) % 10 == 0:
                        print(f"  âœ… {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (í˜„ì¬ ë²ˆí˜¸: {current_number})")
                        sys.stdout.flush()
                        logger.info(f"  âœ… {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (í˜„ì¬ ë²ˆí˜¸: {current_number})")
                    
                    if max_articles and len(articles) >= max_articles:
                        print(f"\n  ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜({max_articles}ê°œ) ë„ë‹¬! í¬ë¡¤ë§ ì¤‘ë‹¨")
                        sys.stdout.flush()
                        logger.info(f"\n  ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜({max_articles}ê°œ) ë„ë‹¬! í¬ë¡¤ë§ ì¤‘ë‹¨")
                        break
                else:
                    consecutive_not_found += 1
                    if consecutive_not_found >= max_consecutive_not_found:
                        print(f"  âš ï¸  ì—°ì† {max_consecutive_not_found}ê°œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í•´ ì¤‘ë‹¨")
                        sys.stdout.flush()
                        logger.info(f"  âš ï¸  ì—°ì† {max_consecutive_not_found}ê°œ ê¸°ì‚¬ë¥¼ ì°¾ì§€ ëª»í•´ ì¤‘ë‹¨")
                        break
                
                current_number -= 1  # ë²ˆí˜¸ë¥¼ 1ì”© ì¤„ì—¬ê°€ë©° íƒìƒ‰ (ìµœì‹  â†’ ê³¼ê±°)
                time.sleep(self.delay)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥ (100ê°œë§ˆë‹¤)
                if total_checked % 100 == 0:
                    print(f"  ğŸ“Š ì§„í–‰ë¥ : {total_checked}ê°œ í™•ì¸, {len(articles)}ê°œ ìˆ˜ì§‘ (í˜„ì¬ ë²ˆí˜¸: {current_number})")
                    sys.stdout.flush()
                    logger.info(f"  ğŸ“Š ì§„í–‰ë¥ : {total_checked}ê°œ í™•ì¸, {len(articles)}ê°œ ìˆ˜ì§‘ (í˜„ì¬ ë²ˆí˜¸: {current_number})")
        else:
            # end_numberê°€ ì§€ì •ëœ ê²½ìš° ë²”ìœ„ ë‚´ì—ì„œ í¬ë¡¤ë§
            while current_number >= end_number:
                article = self.get_article_by_number(category, current_number)
                total_checked += 1
                
                if article:
                    articles.append(article)
                    if len(articles) % 10 == 0:
                        print(f"  âœ… {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (í˜„ì¬ ë²ˆí˜¸: {current_number})")
                        sys.stdout.flush()
                        logger.info(f"  âœ… {len(articles)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ (í˜„ì¬ ë²ˆí˜¸: {current_number})")
                    
                    # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
                    if max_articles and len(articles) >= max_articles:
                        print(f"\n  ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜({max_articles}ê°œ) ë„ë‹¬! í¬ë¡¤ë§ ì¤‘ë‹¨")
                        sys.stdout.flush()
                        logger.info(f"\n  ğŸ¯ ëª©í‘œ ê¸°ì‚¬ ìˆ˜({max_articles}ê°œ) ë„ë‹¬! í¬ë¡¤ë§ ì¤‘ë‹¨")
                        break
                
                current_number -= 1  # ë²ˆí˜¸ë¥¼ 1ì”© ì¤„ì—¬ê°€ë©° íƒìƒ‰ (ìµœì‹  â†’ ê³¼ê±°)
                
                # ì§„í–‰ ìƒí™© ì¶œë ¥ (100ê°œë§ˆë‹¤)
                if total_checked % 100 == 0:
                    progress = ((start_number - current_number) / (start_number - end_number + 1)) * 100
                    print(f"  ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({total_checked}ê°œ í™•ì¸, {len(articles)}ê°œ ìˆ˜ì§‘)")
                    sys.stdout.flush()
                    logger.info(f"  ğŸ“Š ì§„í–‰ë¥ : {progress:.1f}% ({total_checked}ê°œ í™•ì¸, {len(articles)}ê°œ ìˆ˜ì§‘)")
                
                time.sleep(self.delay)
        
        # í¬ë¡¤ë§ ì™„ë£Œ ì¶œë ¥
        print(f"\n{'='*70}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        print(f"{'='*70}")
        print(f"ì´ í™•ì¸í•œ ë²ˆí˜¸: {total_checked}ê°œ")
        print(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
        if total_checked > 0:
            print(f"ìˆ˜ì§‘ë¥ : {len(articles)/total_checked*100:.2f}%")
        print(f"\nìš”ì²­ í†µê³„:")
        print(f"  - ì´ ìš”ì²­: {self.stats['total_requests']}ê°œ")
        print(f"  - ì„±ê³µ: {self.stats['success_count']}ê°œ")
        print(f"  - 404 (ì—†ìŒ): {self.stats['not_found_count']}ê°œ")
        print(f"  - ì—ëŸ¬: {self.stats['error_count']}ê°œ")
        print(f"{'='*70}\n")
        sys.stdout.flush()
        logger.info(f"\n{'='*70}")
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"{'='*70}")
        logger.info(f"ì´ í™•ì¸í•œ ë²ˆí˜¸: {total_checked}ê°œ")
        logger.info(f"ìˆ˜ì§‘ëœ ê¸°ì‚¬: {len(articles)}ê°œ")
        if total_checked > 0:
            logger.info(f"ìˆ˜ì§‘ë¥ : {len(articles)/total_checked*100:.2f}%")
        logger.info(f"\nìš”ì²­ í†µê³„:")
        logger.info(f"  - ì´ ìš”ì²­: {self.stats['total_requests']}ê°œ")
        logger.info(f"  - ì„±ê³µ: {self.stats['success_count']}ê°œ")
        logger.info(f"  - 404 (ì—†ìŒ): {self.stats['not_found_count']}ê°œ")
        logger.info(f"  - ì—ëŸ¬: {self.stats['error_count']}ê°œ")
        logger.info(f"{'='*70}\n")
        
        return articles
    
    def _get_category_name(self, category_slug: str) -> str:
        """ì¹´í…Œê³ ë¦¬ ìŠ¬ëŸ¬ê·¸ë¥¼ í•œê¸€ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        for name, slug in MK_CATEGORIES.items():
            if slug == category_slug:
                return name
        return category_slug
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """ì œëª© ì¶”ì¶œ"""
        title_selectors = [
            '.news_ttl',
            'h1.news_ttl',
            '.article_headline',
            'h1.article_title',
            'h1.title',
            'h1',
            'meta[property="og:title"]'
        ]
        
        for selector in title_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector.startswith('meta'):
                    title = elem.get('content', '').strip()
                else:
                    title = elem.get_text(strip=True)
                if title:
                    return title
        return ""
    
    def _extract_subtitle(self, soup: BeautifulSoup) -> str:
        """ë¶€ì œëª© ì¶”ì¶œ"""
        subtitle_selectors = [
            '.news_sub_ttl',
            '.article_subtitle',
            '.subtitle',
            '.summary'
        ]
        
        for selector in subtitle_selectors:
            elem = soup.select_one(selector)
            if elem:
                subtitle = elem.get_text(strip=True)
                if subtitle:
                    return subtitle
        return ""
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """ë³¸ë¬¸ ì¶”ì¶œ"""
        # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
        for tag in soup.select('script, style, aside, .ad_wrap, .advertisement, .social_share, .comment'):
            tag.decompose()
        
        content_selectors = [
            '.news_cnt_detail_wrap',
            '.article_body',
            '.news_content',
            '.article_content',
            '.content_body',
            'article .content',
            '.story_body',
            '#articleBody'
        ]
        
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = content_elem.get_text(separator='\n', strip=True)
                content = re.sub(r'\n{3,}', '\n\n', content)
                if len(content) > 100:
                    return content
        
        # ë³¸ë¬¸ì„ ì°¾ì§€ ëª»í•œ ê²½ìš° p íƒœê·¸ë“¤ë¡œ ì‹œë„
        paragraphs = soup.select('article p, .article p, .content p')
        if paragraphs:
            content_parts = [p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)]
            content = '\n\n'.join(content_parts)
            if len(content) > 100:
                return content
        
        return ""
    
    def _extract_published_date(self, soup: BeautifulSoup) -> str:
        """ì‘ì„± ì‹œê°„ ì¶”ì¶œ"""
        date_selectors = [
            '.news_date',
            '.article_date',
            '.published_date',
            'time[datetime]',
            '.date',
            'meta[property="article:published_time"]',
            'meta[name="publish-date"]'
        ]
        
        for selector in date_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector.startswith('meta'):
                    date_str = elem.get('content', '').strip()
                elif selector.startswith('time'):
                    date_str = elem.get('datetime', '').strip() or elem.get_text(strip=True)
                else:
                    date_str = elem.get_text(strip=True)
                
                if date_str:
                    return date_str
        return ""
    
    def _extract_reporter(self, soup: BeautifulSoup) -> str:
        """ê¸°ì ì¶”ì¶œ"""
        reporter_selectors = [
            '.journalist_name',
            '.reporter',
            '.author',
            '.writer',
            'meta[name="author"]'
        ]
        
        for selector in reporter_selectors:
            elem = soup.select_one(selector)
            if elem:
                if selector.startswith('meta'):
                    reporter = elem.get('content', '').strip()
                else:
                    reporter = elem.get_text(strip=True)
                if reporter:
                    reporter = re.sub(r'\s+', ' ', reporter).strip()
                    return reporter
        return ""
    
    def _extract_main_image(self, soup: BeautifulSoup) -> str:
        """ë©”ì¸ ì´ë¯¸ì§€ URL ì¶”ì¶œ"""
        # og:image ë©”íƒ€ íƒœê·¸
        og_image = soup.select_one('meta[property="og:image"]')
        if og_image:
            img_url = og_image.get('content', '').strip()
            if img_url:
                return img_url
        
        # ë³¸ë¬¸ì˜ ì²« ë²ˆì§¸ ì´ë¯¸ì§€
        img_selectors = [
            '.news_cnt_detail_wrap img',
            '.article_body img',
            'article img',
            '.content img'
        ]
        
        for selector in img_selectors:
            img = soup.select_one(selector)
            if img and img.get('src'):
                src = img.get('src', '')
                if src.startswith('http'):
                    return src
                elif src.startswith('//'):
                    return 'https:' + src
                else:
                    return urljoin(self.base_url, src)
        
        return ""


class DataStorage:
    """í¬ë¡¤ë§ ë°ì´í„° ì €ì¥ í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "crawled_data"):
        """
        Args:
            output_dir: ì €ì¥ ë””ë ‰í† ë¦¬
        """
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
    
    def save_to_json(self, articles: List[Dict[str, Any]], filename: str = None):
        """
        JSON íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        """
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"mk_news_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(articles, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath} ({len(articles)}ê°œ ê¸°ì‚¬)")
        return filepath
    
    def save_to_csv(self, articles: List[Dict[str, Any]], filename: str = None):
        """
        CSV íŒŒì¼ë¡œ ì €ì¥
        
        Args:
            articles: ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
            filename: íŒŒì¼ëª… (Noneì´ë©´ ìë™ ìƒì„±)
        """
        if not articles:
            logger.warning("ì €ì¥í•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"mk_news_{timestamp}.csv"
        
        filepath = os.path.join(self.output_dir, filename)
        
        # CSV í•„ë“œ ì •ì˜
        fieldnames = ['article_id', 'title', 'subtitle', 'category', 'category_slug', 
                     'reporter', 'published_at', 'url', 'image_url', 'content', 'crawled_at']
        
        with open(filepath, 'w', newline='', encoding='utf-8-sig') as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction='ignore')
            writer.writeheader()
            
            for article in articles:
                writer.writerow(article)
        
        logger.info(f"âœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {filepath} ({len(articles)}ê°œ ê¸°ì‚¬)")
        return filepath


def main():
    parser = argparse.ArgumentParser(
        description='ë§¤ê²½ ë‰´ìŠ¤ ë²ˆí˜¸ ê¸°ë°˜ í¬ë¡¤ëŸ¬',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ê²½ì œ ì¹´í…Œê³ ë¦¬, ìµœì‹  ë²ˆí˜¸ 11485290ë¶€í„° 100ê°œ ìˆ˜ì§‘ (ë²ˆí˜¸ë¥¼ ì¤„ì—¬ê°€ë©° íƒìƒ‰)
  python mk_crawler.py --category economy --start 11485290 --max-articles 100
  
  # ì¦ê¶Œ ì¹´í…Œê³ ë¦¬, ë²ˆí˜¸ ë²”ìœ„ 11484948 ~ 11484000 í¬ë¡¤ë§ (11484948ì—ì„œ 1ì”© ì¤„ì—¬ê°€ë©° 11484000ê¹Œì§€)
  python mk_crawler.py --category stock --start 11484948 --end 11484000
  
  # ê²½ì œ ì¹´í…Œê³ ë¦¬, ìµœì‹  ë²ˆí˜¸ì—ì„œ 100ê°œ ìˆ˜ì§‘ (ë²”ìœ„ ì§€ì • ì—†ìŒ, ë¹ ë¥¸ í¬ë¡¤ë§)
  python mk_crawler.py --category economy --start 11485290 --max-articles 100 --delay 0.2
        """
    )
    
    parser.add_argument('--category', type=str, required=True,
                       help='í¬ë¡¤ë§í•  ì¹´í…Œê³ ë¦¬ (ì˜ˆ: economy, stock, business)')
    parser.add_argument('--start', type=int, required=True,
                       help='ì‹œì‘ ë²ˆí˜¸ (ë§ˆì§€ë§‰ ë²ˆí˜¸, ìµœì‹  ê¸°ì‚¬, í° ë²ˆí˜¸). ì´ ë²ˆí˜¸ì—ì„œë¶€í„° 1ì”© ì¤„ì—¬ê°€ë©° íƒìƒ‰')
    parser.add_argument('--end', type=int, default=None,
                       help='ì¢…ë£Œ ë²ˆí˜¸ (ê³¼ê±° ê¸°ì‚¬, ì‘ì€ ë²ˆí˜¸). ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ max-articlesë§Œí¼ë§Œ ìˆ˜ì§‘')
    parser.add_argument('--max-articles', type=int, default=None,
                       help='ìµœëŒ€ ìˆ˜ì§‘ ê¸°ì‚¬ ìˆ˜ (ë„ë‹¬ ì‹œ ìë™ ì¤‘ë‹¨)')
    parser.add_argument('--delay', type=float, default=0.5,
                       help='ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ) (ê¸°ë³¸: 0.5)')
    parser.add_argument('--output-dir', type=str, default='crawled_data',
                       help='ì €ì¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: crawled_data)')
    
    args = parser.parse_args()
    
    # ì‹œì‘ ë¡œê·¸
    logger.info("=" * 70)
    logger.info("ë§¤ê²½ ë‰´ìŠ¤ í¬ë¡¤ëŸ¬ ì‹œì‘")
    logger.info(f"ë¡œê·¸ íŒŒì¼: {os.path.abspath(log_file)}")
    logger.info("=" * 70)
    
    # ìœ íš¨ì„± ê²€ì‚¬
    if args.category not in MK_CATEGORIES.values():
        error_msg = f"âŒ ì˜¤ë¥˜: '{args.category}' ì¹´í…Œê³ ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n\nì‚¬ìš© ê°€ëŠ¥í•œ ì¹´í…Œê³ ë¦¬:"
        logger.error(error_msg)
        print(error_msg)
        for name, slug in MK_CATEGORIES.items():
            cat_info = f"  {slug:15s} ({name})"
            logger.info(cat_info)
            print(cat_info)
        sys.exit(1)
    
    if args.end is not None and args.start < args.end:
        error_msg = f"âŒ ì˜¤ë¥˜: ì‹œì‘ ë²ˆí˜¸ê°€ ì¢…ë£Œ ë²ˆí˜¸ë³´ë‹¤ ì‘ìŠµë‹ˆë‹¤.\n   ì‹œì‘ ë²ˆí˜¸ëŠ” ì¢…ë£Œ ë²ˆí˜¸ë³´ë‹¤ ì»¤ì•¼ í•©ë‹ˆë‹¤. (í˜„ì¬: {args.start} < {args.end})"
        logger.error(error_msg)
        print(error_msg)
        sys.exit(1)
    
    if args.end is None and args.max_articles is None:
        error_msg = "âŒ ì˜¤ë¥˜: --end ë˜ëŠ” --max-articles ì¤‘ í•˜ë‚˜ëŠ” ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤."
        logger.error(error_msg)
        print(error_msg)
        sys.exit(1)
    
    # í¬ë¡¤ëŸ¬ ìƒì„±
    logger.info(f"í¬ë¡¤ëŸ¬ ì´ˆê¸°í™” ì™„ë£Œ (delay: {args.delay}ì´ˆ)")
    crawler = MKCrawler(delay=args.delay)
    storage = DataStorage(output_dir=args.output_dir)
    
    # í¬ë¡¤ë§ ì‹¤í–‰
    try:
        articles = crawler.crawl_by_number_range(
            category=args.category,
            start_number=args.start,
            end_number=args.end,
            max_articles=args.max_articles
        )
        
        # ê²°ê³¼ ì €ì¥
        if articles:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            category_name = crawler._get_category_name(args.category)
            
            json_file = storage.save_to_json(
                articles, 
                f"mk_news_{args.category}_{timestamp}.json"
            )
            csv_file = storage.save_to_csv(
                articles,
                f"mk_news_{args.category}_{timestamp}.csv"
            )
            
            result_msg = "\n" + "=" * 70 + "\nğŸ“ ì €ì¥ëœ íŒŒì¼:\n"
            result_msg += f"  JSON: {json_file}\n"
            result_msg += f"  CSV: {csv_file}\n"
            result_msg += "=" * 70
            logger.info(result_msg)
            print(result_msg)
            
            # ë²ˆí˜¸ ë²”ìœ„ ì •ë³´
            article_ids = [int(a['article_id']) for a in articles]
            range_msg = f"\nìˆ˜ì§‘ëœ ê¸°ì‚¬ ë²ˆí˜¸ ë²”ìœ„: {min(article_ids)} ~ {max(article_ids)}"
            logger.info(range_msg)
            print(range_msg)
        else:
            no_articles_msg = "\nâŒ ìˆ˜ì§‘ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤."
            logger.warning(no_articles_msg)
            print(no_articles_msg)
            
    except KeyboardInterrupt:
        interrupt_msg = "\n\nâš ï¸  ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤."
        logger.warning(interrupt_msg)
        print(interrupt_msg)
        sys.exit(0)
    except Exception as e:
        error_msg = f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}"
        logger.error(error_msg, exc_info=True)
        print(error_msg)
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
